{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05904d09-ceff-48ef-bb3f-1d033002fac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Download Mobi Bike Share Data\n",
    "\n",
    "- Mobi makes several years of trip data available at [on their website](https://www.mobibikes.ca/en/system-data).\n",
    "- They also adhere to the [GBFS standard](INSERT APPROPRIATE LINK) and have an API available to retrieve live status about stations on the network.\n",
    "- We will also scrape the [Mobi website](INSERT LINK) in case the data is useful to enrich our product later on.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Download and process available trip history (2018-2025, ~7.6M trips).\n",
    "2. Create bronze tables for trips, stations and site data with minimal data maniupulation.\n",
    "3. Examine the contents of the bronze tables and cleanse as appropriate to create silver tables.\n",
    "\n",
    "Runs on Databricks Unity Catalog (Serverless or cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "215d420d-f63d-4e04-81ee-bf4503aa7b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install requests pandas pyarrow beautifulsoup4 openpyxl mlflow markdownify loguru\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook bootstrap: autoreload, src path, imports\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3229a3bf-9abe-4891-9a58-d7fbebf00860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Add the repository's src directory using a relative path\n",
    "src_path = Path.cwd() / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from loguru import logger\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from src.mobi import (\n",
    "    BasicSiteScraper,\n",
    "    combine_trip_data,\n",
    "    download_all_trip_data,\n",
    "    fetch_station_info_from_gbfs,\n",
    "    fetch_station_status_from_gbfs,\n",
    "    save_to_parquet,\n",
    ")\n",
    "\n",
    "CONFIG = mlflow.models.ModelConfig(development_config=\"config.yaml\")\n",
    "\n",
    "CATALOG = CONFIG.get(\"catalog\")\n",
    "SCHEMA = CONFIG.get(\"schema\")\n",
    "RAW_VOLUME = CONFIG.get(\"raw_data_vol\")\n",
    "SHUFFLE_PARTITIONS = CONFIG.get(\"shuffle_partitions\")\n",
    "\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sink=sys.stderr,\n",
    "    level=\"INFO\",\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level:<8} | {message}\",\n",
    ")\n",
    "logger = logger.bind(notebook=\"01_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a141747-10fe-4b17-ae14-55a57d9cbed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Project config & UC paths\n",
    "\n",
    "# Ensure catalog, schema, and volume exist; create if missing\n",
    "# TODO: add check for catalog\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{CATALOG}`.`{SCHEMA}`\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{CATALOG}`.`{SCHEMA}`.`{RAW_VOLUME}`\")\n",
    "\n",
    "# UC Volume root (used by pandas and Spark)\n",
    "volume_root = Path(\"/Volumes\") / CATALOG / SCHEMA / RAW_VOLUME\n",
    "trip_data_dir = volume_root / \"trip_data\"\n",
    "site_data_dir = volume_root / \"mobi_site\"\n",
    "\n",
    "# Spark tuning (shared across all writes)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", SHUFFLE_PARTITIONS)\n",
    "\n",
    "logger.info(\"Using Unity Catalog volume storage\")\n",
    "logger.info(f\"catalog.schema = {CATALOG}.{SCHEMA}\")\n",
    "logger.info(f\"volume = {RAW_VOLUME}\")\n",
    "logger.info(f\"spark.sql.shuffle.partitions = {SHUFFLE_PARTITIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bc301a3-0dfa-46b8-bd99-c18314384959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download Trip Data\n",
    "- Download the trip data to the UC volume you created and combine into a single file.\n",
    "- We'll use this to create bronze tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a30478-a1d2-428c-9fd4-6aa56bff4d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download all files\n",
    "raw_trip_dir = trip_data_dir / \"raw\"\n",
    "logger.info(f\"Downloading trip data to {raw_trip_dir}\")\n",
    "files = download_all_trip_data(raw_trip_dir)\n",
    "logger.success(f\"Downloaded {len(files)} files\")\n",
    "\n",
    "# Process and combine\n",
    "logger.info(\"Combining downloaded trip extracts\")\n",
    "trips_df = combine_trip_data(files)\n",
    "logger.success(f\"Total trips processed: {len(trips_df):,}\")\n",
    "\n",
    "# Save\n",
    "trips_output = trip_data_dir / \"mobi_trips.parquet\"\n",
    "logger.info(f\"Writing trips parquet to {trips_output}\")\n",
    "save_to_parquet(trips_df, trips_output)\n",
    "logger.success(\"Trips parquet saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e55c2d-0ac9-46e8-9fe9-5183ece1308f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download Station Data\n",
    "\n",
    "We now turn our attention to the station data using GBFS (General Bikeshare Feed Specification), the gold standard for bikeshare system info. ðŸš²\n",
    "\n",
    "**Whatâ€™s happening here?**\n",
    "- Weâ€™ll fetch live station details (think: locations, capacities, and real-time availability) straight from the GBFS feeds.\n",
    "- By merging inventory and status, we get both *where* stations are and *whatâ€™s going on* at each one.\n",
    "- We save it all into our Unity Catalog volume for downstream analytics magic.\n",
    "\n",
    "**Data cleansing is a critical piece of any ML initiative - some of these files won't load as expected. It's up to you to decide if and what to do about that!**\n",
    "\n",
    "_Hint: are we sure every file is a csv? ðŸ¤”_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492c8ba0-660d-4e84-97c1-9a793e493b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch from GBFS API\n",
    "logger.info(\"Fetching station inventory and status from GBFS\")\n",
    "stations = fetch_station_info_from_gbfs()\n",
    "status = fetch_station_status_from_gbfs()\n",
    "\n",
    "# Combine\n",
    "stations = stations.merge(status, on=\"station_id\", how=\"left\")\n",
    "logger.success(f\"Merged station details: {len(stations)} rows\")\n",
    "\n",
    "# Save\n",
    "stations_parquet = trip_data_dir / \"mobi_stations.parquet\"\n",
    "stations_csv = trip_data_dir / \"mobi_stations.csv\"\n",
    "logger.info(f\"Writing stations parquet to {stations_parquet}\")\n",
    "save_to_parquet(stations, stations_parquet)\n",
    "logger.info(f\"Exporting stations CSV to {stations_csv}\")\n",
    "stations.to_csv(stations_csv, index=False)\n",
    "logger.success(\"Station datasets updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f3539a-94cb-4121-8f38-61432bb177f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Scrape mobi Webiste\n",
    "- Our users may want to ask detailed questions about how to use Mobi bike share. We can scrape their website to get the best and most current available info about the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8f25da-98bf-4070-9ae2-2e13ad0b0131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure scraper\n",
    "BASE_URL = CONFIG.get(\"scrape_url\")\n",
    "START_URL = BASE_URL\n",
    "SCRAPE_DELAY = float(CONFIG.get(\"scrape_delay\"))\n",
    "SCRAPE_MAX_DEPTH = int(CONFIG.get(\"scrape_max_depth\"))\n",
    "\n",
    "scraper = BasicSiteScraper(\n",
    "    base_url=BASE_URL,\n",
    "    delay=SCRAPE_DELAY,\n",
    "    max_depth=SCRAPE_MAX_DEPTH,\n",
    ")\n",
    "\n",
    "logger.info(f\"Scraping mobi site starting at {START_URL}\")\n",
    "pages = scraper.scrape_recursive(START_URL)\n",
    "logger.success(f\"Pages scraped: {len(pages)}\")\n",
    "preview_urls = list(pages.keys())[: min(10, len(pages))]\n",
    "logger.info(f\"Previewing first {len(preview_urls)} URLs: {preview_urls}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2dd97a9-3a92-409f-a38f-fb3dc20722fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save markdown content per page\n",
    "raw_site_data_dir = site_data_dir / \"raw\"\n",
    "raw_site_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Persisting raw site markdown to {raw_site_data_dir}\")\n",
    "\n",
    "def url_to_filename(url: str) -> str:\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.rstrip(\"/\") or \"index\"\n",
    "    safe = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", path)\n",
    "    if not safe.endswith(\".md\"):\n",
    "        safe += \".md\"\n",
    "    return safe\n",
    "\n",
    "count = 0\n",
    "for url, data in pages.items():\n",
    "    filepath = raw_site_data_dir / url_to_filename(url)\n",
    "    filepath.write_text(data.get(\"content\", \"\"), encoding=\"utf-8\")\n",
    "    count += 1\n",
    "\n",
    "logger.success(f\"Wrote {count} markdown files to {raw_site_data_dir}\")\n",
    "\n",
    "# Save to parquet\n",
    "records = []\n",
    "for url, data in pages.items():\n",
    "    md = data.get(\"metadata\", {})\n",
    "    records.append({\n",
    "        \"url\": url,\n",
    "        \"title\": md.get(\"title\", \"\"),\n",
    "        \"description\": md.get(\"description\", \"\"),\n",
    "        \"main_heading\": md.get(\"main_heading\", \"\"),\n",
    "        \"scraped_at\": md.get(\"scraped_at\", None),\n",
    "        \"status\": data.get(\"status\", \"\"),\n",
    "        \"content_md\": data.get(\"content\", \"\"),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "output_path = site_data_dir / \"mobibikes_ca_content.parquet\"\n",
    "logger.info(f\"Saving site content parquet to {output_path}\")\n",
    "save_to_parquet(df, output_path)\n",
    "logger.success(\"Site content parquet saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17347f2d-d4d9-40ea-a5eb-2a8f76da9e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd193f05-f988-46c2-8821-eeab66fa06ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "trips = pd.read_parquet(trip_data_dir / \"mobi_trips.parquet\")\n",
    "stations = pd.read_parquet(trip_data_dir / \"mobi_stations.parquet\")\n",
    "mobi_site = pd.read_parquet(site_data_dir / \"mobibikes_ca_content.parquet\")\n",
    "\n",
    "logger.success(f\"Trips loaded: {len(trips):,}\")\n",
    "logger.success(f\"Stations loaded: {len(stations)}\")\n",
    "logger.success(f\"Site pages available: {len(mobi_site)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf5664b0-a241-441f-ba41-79dc3baca2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Bronze Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656ee11b-627c-46bf-bf68-78b7cb529686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trips_pq = str(trip_data_dir / \"mobi_trips.parquet\")\n",
    "stations_pq = str(trip_data_dir / \"mobi_stations.parquet\")\n",
    "site_pq = str(site_data_dir / \"mobibikes_ca_content.parquet\")\n",
    "\n",
    "(\n",
    "    spark.read.parquet(trips_pq)\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", True)\n",
    "    .saveAsTable(f'`{CATALOG}`.`{SCHEMA}`.`bronze_trips`')\n",
    ")\n",
    "\n",
    "(\n",
    "    spark.read.parquet(stations_pq)\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", True)\n",
    "    .saveAsTable(f'`{CATALOG}`.`{SCHEMA}`.`bronze_stations`')\n",
    ")\n",
    "\n",
    "(\n",
    "    spark.read.parquet(site_pq)\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", True)\n",
    "    .saveAsTable(f'`{CATALOG}`.`{SCHEMA}`.`bronze_site`')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2a9131-bf81-4617-bc22-ece172fdb0cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Silver Trips\n",
    "\n",
    "Our bronze trips table mirrors the raw CSVs, so it still contains duplicate columns, misspellings, and inconsistent station labels. In the next steps we will transform that raw feed into a clean silver layer that downstream analysts can join to stations and sites without extra work.\n",
    "\n",
    "Key goals for the silver pipeline:\n",
    "- **Normalize columns** â€“ coalesce duplicate names (for example `membership_type` and `memebership_type`), convert metrics to numeric types, and ensure timestamps are cast correctly.\n",
    "- **Derive trusted flags** â€“ consolidate the electric-bike indicators into a single boolean field.\n",
    "- **Standardize station names** â€“ strip numeric prefixes, collapse whitespace, and attach station IDs for both departure and return locations.\n",
    "- **Assign primary keys** â€“ create a deterministic `trip_id` window function so every trip row has a unique identifier.\n",
    "\n",
    "!!SCREENSHOT: BRONZE TRIPS SAMPLE (OPTIONAL)!!\n",
    "\n",
    "> **Reminder:** Each transformation cell that follows displays a sample of the intermediate DataFrame so you can confirm the changes before writing the silver tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect bronze tables before silver transforms\n",
    "\n",
    "def summarize_columns(df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a sorted DataFrame detailing column names, dtypes, and null counts.\"\"\"\n",
    "    summary = pd.DataFrame(\n",
    "        {\n",
    "            \"column\": df.columns,\n",
    "            \"dtype\": df.dtypes.astype(str),\n",
    "            \"null_count\": df.isna().sum(),\n",
    "        }\n",
    "    )\n",
    "    logger.info(f\"{label} has {len(df):,} rows\")\n",
    "    return summary.sort_values(\"column\").reset_index(drop=True)\n",
    "\n",
    "bronze_trips_summary = summarize_columns(trips, \"bronze_trips\")\n",
    "bronze_stations_summary = summarize_columns(stations, \"bronze_stations\")\n",
    "bronze_site_summary = summarize_columns(mobi_site, \"bronze_site\")\n",
    "\n",
    "display(bronze_trips_summary)\n",
    "display(bronze_stations_summary)\n",
    "display(bronze_site_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper utilities for silver preparation\n",
    "\n",
    "def clean_station_label_col(column: F.Column) -> F.Column:\n",
    "    \"\"\"Return a Spark column with 4-digit numeric prefixes removed and whitespace normalized.\"\"\"\n",
    "    return F.regexp_replace(\n",
    "        F.regexp_replace(F.trim(column), r\"^\\s*\\d{4}[\\s\\-:]*\", \"\"), r\"\\s+\", \" \"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_station_key_col(column: F.Column) -> F.Column:\n",
    "    \"\"\"Create a lowercase, single-space station key suitable for joining.\"\"\"\n",
    "    return F.lower(F.regexp_replace(F.trim(column), r\"\\s+\", \" \"))\n",
    "\n",
    "\n",
    "def coerce_flag_col(column: F.Column) -> F.Column:\n",
    "    \"\"\"Convert yes/no style strings to a nullable boolean Spark column.\n",
    "\n",
    "    TODO: Extend with additional localized values if they appear in new extracts.\n",
    "    \"\"\"\n",
    "    normalized = F.lower(F.trim(column))\n",
    "    truthy = [\"yes\", \"true\", \"1\", \"y\"]\n",
    "    falsy = [\"no\", \"false\", \"0\", \"n\"]\n",
    "    return (\n",
    "        F.when(normalized.isin(*truthy), F.lit(True))\n",
    "        .when(normalized.isin(*falsy), F.lit(False))\n",
    "        .otherwise(F.lit(None))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare silver_stations DataFrame using Spark\n",
    "bronze_stations_sdf = spark.table(f'`{CATALOG}`.`{SCHEMA}`.`bronze_stations`')\n",
    "\n",
    "silver_stations = (\n",
    "    bronze_stations_sdf\n",
    "    .withColumn(\"station_name\", clean_station_label_col(F.col(\"name\")))\n",
    "    .withColumn(\"station_key\", normalize_station_key_col(F.col(\"name\")))\n",
    "    .withColumn(\"station_pk\", F.col(\"station_id\"))\n",
    "    .dropDuplicates([\"station_id\"])\n",
    ")\n",
    "# TODO: capture additional station metadata (e.g., capacity) if required downstream.\n",
    "logger.success(f\"Prepared silver_stations with {silver_stations.count()} rows\")\n",
    "display(silver_stations.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bronze trips as the starting point for the silver transformation\n",
    "bronze_trips_sdf = spark.table(f'`{CATALOG}`.`{SCHEMA}`.`bronze_trips`')\n",
    "silver_trips = bronze_trips_sdf\n",
    "logger.info(f\"Loaded bronze_trips rows: {silver_trips.count()}\")\n",
    "display(silver_trips.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Coalesce duplicate membership columns\n",
    "if \"memebership_type\" in bronze_trips_sdf.columns:\n",
    "    if \"membership_type\" in silver_trips.columns:\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            \"membership_type\",\n",
    "            F.coalesce(F.col(\"membership_type\"), F.col(\"memebership_type\")),\n",
    "        )\n",
    "    else:\n",
    "        silver_trips = silver_trips.withColumn(\"membership_type\", F.col(\"memebership_type\"))\n",
    "    silver_trips = silver_trips.drop(\"memebership_type\")\n",
    "\n",
    "logger.info(f\"After membership cleanup: {silver_trips.count()} rows\")\n",
    "display(silver_trips.select(\"membership_type\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize stopover duration values\n",
    "if \"stopover_duration\" in silver_trips.columns:\n",
    "    if \"stopover_duration_sec\" in silver_trips.columns:\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            \"stopover_duration_sec\",\n",
    "            F.coalesce(F.col(\"stopover_duration_sec\"), F.col(\"stopover_duration\")),\n",
    "        )\n",
    "    else:\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            \"stopover_duration_sec\", F.col(\"stopover_duration\").cast(\"double\")\n",
    "        )\n",
    "    silver_trips = silver_trips.drop(\"stopover_duration\")\n",
    "\n",
    "logger.info(f\"After stopover normalization: {silver_trips.count()} rows\")\n",
    "display(silver_trips.select(\"stopover_duration_sec\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply numeric casts and derived distance\n",
    "numeric_casts = {\n",
    "    \"bike_id\": \"bigint\",\n",
    "    \"covered_distance_m\": \"double\",\n",
    "    \"duration_sec\": \"double\",\n",
    "    \"stopover_duration_sec\": \"double\",\n",
    "    \"number_of_stopovers\": \"double\",\n",
    "    \"departure_slot\": \"double\",\n",
    "    \"return_slot\": \"double\",\n",
    "    \"lock_duration_sec\": \"double\",\n",
    "    \"number_of_bike_locks\": \"double\",\n",
    "    \"number_of_bike_stopovers\": \"double\",\n",
    "    \"departure_battery_voltage_mv\": \"double\",\n",
    "    \"return_battery_voltage_mv\": \"double\",\n",
    "}\n",
    "for column, dtype in numeric_casts.items():\n",
    "    if column in silver_trips.columns:\n",
    "        silver_trips = silver_trips.withColumn(column, F.col(column).cast(dtype))\n",
    "\n",
    "if \"covered_distance_m\" in silver_trips.columns:\n",
    "    silver_trips = silver_trips.withColumn(\n",
    "        \"covered_distance_km\", F.col(\"covered_distance_m\") / F.lit(1000.0)\n",
    "    )\n",
    "\n",
    "logger.info(f\"After numeric coercion: {silver_trips.count()} rows\")\n",
    "display(silver_trips.select(*[c for c in [\"covered_distance_m\", \"covered_distance_km\", \"duration_sec\"] if c in silver_trips.columns]).limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Cast departure and return timestamps (source data already hourly)\n",
    "# Handle slash and dash formats plus minute-only timestamps without seconds\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Pandas UDF fallback\n",
    "@pandas_udf(TimestampType())\n",
    "def infer_ts(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"Fallback parser using pandas.to_datetime() with inference.\"\"\"\n",
    "    return pd.to_datetime(col, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "# Fast native parse\n",
    "def fast_timestamp(col):\n",
    "    \"\"\"Lightweight parser for most rows.\"\"\"\n",
    "    c = F.trim(F.col(col))\n",
    "    c = F.regexp_replace(c, r\"[./]\", \"-\")  # normalize separators\n",
    "    # reorder M-D-Y -> Y-M-D\n",
    "    c = F.regexp_replace(c, r\"^([0-9]{1,2})-([0-9]{1,2})-([0-9]{4})\", r\"\\3-\\1-\\2\")\n",
    "    return F.try_to_timestamp(c)\n",
    "\n",
    "for raw, target in [(\"departure\", \"departure_time\"), (\"return\", \"return_time\")]:\n",
    "    if raw in silver_trips.columns:\n",
    "        # fast native parse\n",
    "        silver_trips = silver_trips.withColumn(target, fast_timestamp(raw))\n",
    "        # pandas fallback only for nulls\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            target,\n",
    "            F.when(F.col(target).isNull(), infer_ts(F.col(raw))).otherwise(F.col(target))\n",
    "        )\n",
    "        silver_trips = silver_trips.drop(raw)\n",
    "\n",
    "for colname in [\"departure_time\", \"return_time\"]:\n",
    "    if colname in silver_trips.columns:\n",
    "        silver_trips = silver_trips.withColumn(colname, fast_timestamp(colname))\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            colname,\n",
    "            F.when(F.col(colname).isNull(), infer_ts(F.col(colname))).otherwise(F.col(colname))\n",
    "        )\n",
    "\n",
    "# Add date based partitioning columns for later\n",
    "if \"departure_time\" in silver_trips.columns:\n",
    "    silver_trips = (\n",
    "        silver_trips\n",
    "        .withColumn(\"departure_year\",  F.year(\"departure_time\"))\n",
    "        .withColumn(\"departure_month\", F.month(\"departure_time\"))\n",
    "    )\n",
    "\n",
    "display(silver_trips.select(\"departure_time\", \"return_time\", \"departure_year\", \"departure_month\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Consolidate electric bike indicators (coalesce known boolean columns)\n",
    "flag_columns = [c for c in [\"electric_bike\", \"electric\"] if c in silver_trips.columns]\n",
    "\n",
    "if flag_columns:\n",
    "    silver_trips = silver_trips.withColumn(\n",
    "        \"is_electric_bike\",\n",
    "        F.coalesce(*[F.col(c).cast(\"boolean\") for c in flag_columns]),\n",
    "    )\n",
    "else:\n",
    "    silver_trips = silver_trips.withColumn(\"is_electric_bike\", F.lit(None).cast(\"boolean\"))\n",
    "\n",
    "for column in flag_columns:\n",
    "    silver_trips = silver_trips.drop(column)\n",
    "\n",
    "logger.info(f\"After electric flag normalization: {silver_trips.count()} rows\")\n",
    "display(silver_trips.select(\"is_electric_bike\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Clean station names (strip numeric prefixes, collapse whitespace)\n",
    "for prefix in [\"departure\", \"return\"]:\n",
    "    raw_column = f\"{prefix}_station\"\n",
    "    if raw_column in silver_trips.columns:\n",
    "        cleaned = clean_station_label_col(F.col(raw_column))\n",
    "        silver_trips = silver_trips.withColumn(f\"{prefix}_station_name\", cleaned)\n",
    "        silver_trips = silver_trips.drop(raw_column)\n",
    "\n",
    "logger.info(f\"After station name cleanup: {silver_trips.count()} rows\")\n",
    "display(\n",
    "    silver_trips.select(\n",
    "        *[\n",
    "            c\n",
    "            for c in [\n",
    "                \"departure_station_name\",\n",
    "                \"return_station_name\",\n",
    "            ]\n",
    "            if c in silver_trips.columns\n",
    "        ]\n",
    "    ).limit(5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Attach station IDs using normalized names\n",
    "dep_lookup = silver_stations.select(\n",
    "    F.col(\"station_key\").alias(\"dep_station_key\"),\n",
    "    F.col(\"station_id\").alias(\"departure_station_id\"),\n",
    ")\n",
    "ret_lookup = silver_stations.select(\n",
    "    F.col(\"station_key\").alias(\"ret_station_key\"),\n",
    "    F.col(\"station_id\").alias(\"return_station_id\"),\n",
    ")\n",
    "\n",
    "silver_trips = silver_trips.join(\n",
    "    dep_lookup,\n",
    "    normalize_station_key_col(F.col(\"departure_station_name\")) == dep_lookup[\"dep_station_key\"],\n",
    "    \"left\",\n",
    ").drop(\"dep_station_key\")\n",
    "\n",
    "silver_trips = silver_trips.join(\n",
    "    ret_lookup,\n",
    "    normalize_station_key_col(F.col(\"return_station_name\")) == ret_lookup[\"ret_station_key\"],\n",
    "    \"left\",\n",
    ").drop(\"ret_station_key\")\n",
    "\n",
    "logger.info(f\"After station ID join: {silver_trips.count()} rows\")\n",
    "display(\n",
    "    silver_trips.select(\n",
    "        *[\n",
    "            c\n",
    "            for c in [\n",
    "                \"departure_station_name\",\n",
    "                \"departure_station_id\",\n",
    "                \"return_station_name\",\n",
    "                \"return_station_id\",\n",
    "            ]\n",
    "            if c in silver_trips.columns\n",
    "        ]\n",
    "    ).limit(5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Finalize trip_id and column ordering (simple fast version)\n",
    "\n",
    "# Generate a unique ID per row\n",
    "silver_trips = silver_trips.withColumn(\"trip_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Preserve original column ordering logic\n",
    "trip_columns_order = [\n",
    "    \"trip_id\",\n",
    "    \"departure_time\",\n",
    "    \"return_time\",\n",
    "    \"bike\",\n",
    "    \"is_electric_bike\",\n",
    "    \"membership_type\",\n",
    "    \"covered_distance_m\",\n",
    "    \"covered_distance_km\",\n",
    "    \"duration_sec\",\n",
    "    \"stopover_duration_sec\",\n",
    "    \"number_of_stopovers\",\n",
    "    \"number_of_bike_stopovers\",\n",
    "    \"departure_slot\",\n",
    "    \"return_slot\",\n",
    "    \"lock_duration_sec\",\n",
    "    \"number_of_bike_locks\",\n",
    "    \"departure_temperature_c\",\n",
    "    \"return_temperature_c\",\n",
    "    \"departure_battery_voltage_mv\",\n",
    "    \"return_battery_voltage_mv\",\n",
    "    \"departure_station_name\",\n",
    "    \"departure_station_id\",\n",
    "    \"return_station_name\",\n",
    "    \"return_station_id\",\n",
    "    \"source_file\",\n",
    "    \"formula\",\n",
    "    \"manager\",\n",
    "]\n",
    "\n",
    "existing_trip_columns = [c for c in trip_columns_order if c in silver_trips.columns]\n",
    "remaining_columns = [c for c in silver_trips.columns if c not in existing_trip_columns]\n",
    "silver_trips = silver_trips.select(*(existing_trip_columns + remaining_columns))\n",
    "\n",
    "logger.success(f\"Prepared silver_trips with {silver_trips.count()} rows\")\n",
    "display(silver_trips.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare silver_site DataFrame and assign primary key\n",
    "bronze_site_sdf = spark.table(f'`{CATALOG}`.`{SCHEMA}`.`bronze_site`')\n",
    "\n",
    "site_window = Window.orderBy(\"url\")\n",
    "silver_site = (\n",
    "    bronze_site_sdf\n",
    "    .withColumn(\"scraped_at\", F.to_timestamp(\"scraped_at\"))\n",
    "    .dropDuplicates([\"url\"])\n",
    "    .withColumn(\"site_page_id\", F.row_number().over(site_window).cast(\"bigint\"))\n",
    ")\n",
    "# TODO: revisit primary key strategy if additional natural keys become available.\n",
    "\n",
    "logger.success(f\"Prepared silver_site with {silver_site.count()} pages\")\n",
    "display(silver_site.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist silver tables to Unity Catalog Delta\n",
    "\n",
    "logger.info(\"Repartitioning silver_trips by year and month\")\n",
    "silver_trips = silver_trips.repartition(\"departure_year\", \"departure_month\")\n",
    "\n",
    "# Drop existing tables (if any) to avoid slow schema reconciliation\n",
    "for t in [\"silver_trips\", \"silver_stations\", \"silver_site\"]:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{t}`\")\n",
    "\n",
    "logger.info(f\"Writing silver_trips to `{CATALOG}`.`{SCHEMA}`.silver_trips\")\n",
    "(\n",
    "    silver_trips.write\n",
    "    .format(\"delta\")\n",
    "    .partitionBy(\"departure_year\", \"departure_month\")\n",
    "    .saveAsTable(f\"`{CATALOG}`.`{SCHEMA}`.`silver_trips`\")\n",
    ")\n",
    "\n",
    "logger.info(f\"Writing silver_stations to `{CATALOG}`.`{SCHEMA}`.silver_stations\")\n",
    "(\n",
    "    silver_stations.write\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(f\"`{CATALOG}`.`{SCHEMA}`.`silver_stations`\")\n",
    ")\n",
    "\n",
    "logger.info(f\"Writing silver_site to `{CATALOG}`.`{SCHEMA}`.silver_site\")\n",
    "(\n",
    "    silver_site.write\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(f\"`{CATALOG}`.`{SCHEMA}`.`silver_site`\")\n",
    ")\n",
    "\n",
    "logger.success(\"Silver tables materialized\")\n",
    "\n",
    "silver_trips.printSchema()\n",
    "silver_stations.printSchema()\n",
    "silver_site.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7783426245318998,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_data",
   "widgets": {
    "catalog": {
     "currentValue": "max-howarth-demos",
     "nuid": "448cb431-98ac-4324-9cb5-efc894163b7b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "max-howarth-demos",
      "label": null,
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "max-howarth-demos",
      "label": null,
      "name": "catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema": {
     "currentValue": "vancouver-hackathon-1125",
     "nuid": "87d9a43b-d3b1-4821-9fa2-23d4f8ff5c73",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "vancouver-hackathon-1125",
      "label": null,
      "name": "schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "vancouver-hackathon-1125",
      "label": null,
      "name": "schema",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "shuffle_partitions": {
     "currentValue": "64",
     "nuid": "bd16e9ad-da3f-45eb-9465-315808eff8a9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "64",
      "label": null,
      "name": "shuffle_partitions",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "64",
      "label": null,
      "name": "shuffle_partitions",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "volume": {
     "currentValue": "raw_data",
     "nuid": "b58e970b-9d26-4174-8c0f-695598aa54fe",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "raw_data",
      "label": null,
      "name": "volume",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "raw_data",
      "label": null,
      "name": "volume",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
