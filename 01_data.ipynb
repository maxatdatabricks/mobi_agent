{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05904d09-ceff-48ef-bb3f-1d033002fac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Download Mobi Bike Share Data\n",
    "\n",
    "- Mobi makes several years of trip data available at [on their website](https://www.mobibikes.ca/en/system-data).\n",
    "- They also adhere to the [GBFS standard](INSERT APPROPRIATE LINK) and have an API available to retrieve live status about stations on the network.\n",
    "- We will also scrape the [Mobi website](INSERT LINK) in case the data is useful to enrich our product later on.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Download and process available trip history (2018-2025, ~7.6M trips).\n",
    "2. Create bronze tables for trips, stations and site data with minimal data maniupulation.\n",
    "3. Examine the contents of the bronze tables and cleanse as appropriate to create silver tables.\n",
    "\n",
    "Runs on Databricks Unity Catalog (Serverless or cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2713f33-5631-414b-9ad4-5dbd5a0484f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Before running this notebook, ensure your `config.yaml` is correctly configured with:\n",
    "\n",
    "- `catalog` â€” your Unity Catalog catalog  \n",
    "- `schema` â€” the schema you will write tables into  \n",
    "- `raw_data_vol` â€” the UC volume where raw data files will be stored  \n",
    "- Optional scraper parameters for Mobi website content  \n",
    "\n",
    "This notebook expects that:\n",
    "\n",
    "- You are using **Databricks Serverless**\n",
    "- You are working entirely inside **Unity Catalog**\n",
    "- All data is stored using **UC Volumes** and **Delta Tables**\n",
    "\n",
    "### Overwrite Behavior\n",
    "\n",
    "Many cells in this notebook use **overwrite mode** when writing into the RAW volume or Delta tables.\n",
    "\n",
    "This is intentional and safe for the hackathon:\n",
    "\n",
    "- If you re-download the data or fix a transformation, **overwrite ensures your tables stay consistent**.\n",
    "- It prevents half-written or mixed-version datasets caused by incremental writes.\n",
    "- It allows teams to restart from scratch without manually clearing volumes or tables.\n",
    "\n",
    "**If you want to preserve an older version**, simply rename the table before re-running.\n",
    "\n",
    "During the hackathon, feel free to run this notebook multiple times â€” it is designed to be **idempotent** and safe to rerun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "215d420d-f63d-4e04-81ee-bf4503aa7b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install requests pandas pyarrow beautifulsoup4 openpyxl mlflow markdownify loguru\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8be446c0-7fd5-46df-8297-677b7b0c5b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook bootstrap: autoreload, src path, imports\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3229a3bf-9abe-4891-9a58-d7fbebf00860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Add the repository's src directory using a relative path\n",
    "src_path = Path.cwd() / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from loguru import logger\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "from src.mobi import (\n",
    "    BasicSiteScraper,\n",
    "    MobiDataDownloaderError,\n",
    "    StationDataError,\n",
    "    combine_trip_data,\n",
    "    download_all_trip_data,\n",
    "    fetch_station_info_from_gbfs,\n",
    "    fetch_station_status_from_gbfs,\n",
    "    seed_volume_from_backup,\n",
    "    save_to_parquet,\n",
    ")\n",
    "\n",
    "CONFIG = mlflow.models.ModelConfig(development_config=\"config.yaml\")\n",
    "\n",
    "CATALOG = CONFIG.get(\"catalog\")\n",
    "SCHEMA = CONFIG.get(\"schema\")\n",
    "RAW_VOLUME = CONFIG.get(\"raw_data_vol\")\n",
    "SHUFFLE_PARTITIONS = CONFIG.get(\"shuffle_partitions\")\n",
    "\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sink=sys.stderr,\n",
    "    level=\"INFO\",\n",
    "    format=\"{time:YYYY-MM-DD HH:mm:ss} | {level:<8} | {message}\",\n",
    ")\n",
    "logger = logger.bind(notebook=\"01_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "479881ec-d612-48b5-a86d-1cb3b73dff34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OVERWRITE_DOWNLOADS = bool(CONFIG.get(\"overwrite_downloads\"))\n",
    "BUNDLE_PATH = Path.cwd() / \"data.zip\"\n",
    "logger.info(f\"OVERWRITE_DOWNLOADS set to {OVERWRITE_DOWNLOADS}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a141747-10fe-4b17-ae14-55a57d9cbed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Project config & UC paths\n",
    "\n",
    "# Ensure catalog, schema, and volume exist; create if missing\n",
    "# TODO: add check for catalog\n",
    "try:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{CATALOG}`.`{SCHEMA}`\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Have you created the catalog? {CATALOG}\")\n",
    "    logger.error(f\"Error creating schema: {e}\")\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS `{CATALOG}`.`{SCHEMA}`.`{RAW_VOLUME}`\")\n",
    "\n",
    "# UC Volume root (used by pandas and Spark)\n",
    "volume_root = Path(\"/Volumes\") / CATALOG / SCHEMA / RAW_VOLUME\n",
    "trip_data_dir = volume_root / \"trip_data\"\n",
    "site_data_dir = volume_root / \"mobi_site\"\n",
    "\n",
    "# Spark tuning (shared across all writes)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", SHUFFLE_PARTITIONS)\n",
    "\n",
    "logger.info(\"Using Unity Catalog volume storage\")\n",
    "logger.info(f\"catalog.schema = {CATALOG}.{SCHEMA}\")\n",
    "logger.info(f\"volume = {RAW_VOLUME}\")\n",
    "logger.info(f\"spark.sql.shuffle.partitions = {SHUFFLE_PARTITIONS}\")\n",
    "\n",
    "seed_volume_from_backup(volume_root, BUNDLE_PATH)\n",
    "logger.info(\"Seeded volume with bundled trip and site data (non-destructive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bc301a3-0dfa-46b8-bd99-c18314384959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download Trip Data\n",
    "\n",
    "Here we pull historical trip data from the public Mobi open data portal and store it inside your configured **RAW** UC volume.\n",
    "\n",
    "This step:\n",
    "\n",
    "- Retrieves all published trip CSVs.\n",
    "- Ensures consistent naming and storage inside UC.\n",
    "- Falls back to a bundled archive if downloads are unavailable (e.g., network issues during the hackathon).\n",
    "\n",
    "Once this completes, you should:\n",
    "\n",
    "- Browse the RAW volume to confirm the CSV files exist.\n",
    "- Validate a few rows using `spark.read.csv` if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a30478-a1d2-428c-9fd4-6aa56bff4d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download all files\n",
    "raw_trip_dir = trip_data_dir / \"raw\"\n",
    "raw_trip_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if OVERWRITE_DOWNLOADS:\n",
    "    logger.info(f\"Downloading trip data to {raw_trip_dir}\")\n",
    "    files = download_all_trip_data(raw_trip_dir, overwrite=True)\n",
    "    logger.success(f\"Downloaded {len(files)} files\")\n",
    "else:\n",
    "    logger.info(\n",
    "        \"OVERWRITE_DOWNLOADS is False; using pre-seeded trip extracts instead of downloading\"\n",
    "    )\n",
    "    files = sorted(raw_trip_dir.glob(\"*.csv\"))\n",
    "    if not files:\n",
    "        raise RuntimeError(\n",
    "            \"No trip CSV files available in the seeded backup. Set OVERWRITE_DOWNLOADS=True to fetch fresh data.\"\n",
    "        )\n",
    "    logger.success(f\"Found {len(files)} pre-seeded trip files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fe4ebcd-6c05-4167-b862-d7c12be678a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process and combine\n",
    "logger.info(\"Combining downloaded trip extracts\")\n",
    "trips_df = combine_trip_data(files)\n",
    "logger.success(f\"Total trips processed: {len(trips_df):,}\")\n",
    "\n",
    "# Save\n",
    "trips_output = trip_data_dir / \"mobi_trips.parquet\"\n",
    "logger.info(f\"Writing trips parquet to {trips_output}\")\n",
    "save_to_parquet(trips_df, trips_output)\n",
    "logger.success(\"Trips parquet saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37e55c2d-0ac9-46e8-9fe9-5183ece1308f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download Station Data\n",
    "\n",
    "Next we turn to **station-level data** using GBFS (General Bikeshare Feed Specification), the standard for bike share system metadata and status feeds. ðŸš²\n",
    "\n",
    "In this section you will:\n",
    "\n",
    "- Call the Mobi GBFS endpoints to retrieve:\n",
    "  - **Station information** (IDs, names, locations, capacities).\n",
    "  - **Station status** (bikes available, docks available, etc.).\n",
    "- Combine inventory and status so you know both *where* stations are and *whatâ€™s going on* at each one.\n",
    "- Persist these snapshots into your RAW volume or directly into a bronze Delta table.\n",
    "\n",
    "This gives your team:\n",
    "\n",
    "- A foundation for **availability modelling**, **rebalancing logic**, or **real-time insights**.\n",
    "- Station attributes that you can join to trip data and scraped site content.\n",
    "\n",
    "> Data quality is part of the challenge: check for missing values, unusual stations, or files that donâ€™t match the expected format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "492c8ba0-660d-4e84-97c1-9a793e493b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fetch from GBFS API\n",
    "logger.info(\"Preparing station inventory and status data\")\n",
    "stations_parquet = trip_data_dir / \"mobi_stations.parquet\"\n",
    "stations_csv = trip_data_dir / \"mobi_stations.csv\"\n",
    "\n",
    "if OVERWRITE_DOWNLOADS:\n",
    "    logger.info(\"OVERWRITE_DOWNLOADS is True; fetching stations from GBFS\")\n",
    "    station_info = fetch_station_info_from_gbfs()\n",
    "    station_status = fetch_station_status_from_gbfs()\n",
    "    stations = station_info.merge(station_status, on=\"station_id\", how=\"left\")\n",
    "    logger.success(f\"Merged station details: {len(stations)} rows\")\n",
    "\n",
    "    logger.info(f\"Writing stations parquet to {stations_parquet}\")\n",
    "    save_to_parquet(stations, stations_parquet)\n",
    "    logger.info(f\"Exporting stations CSV to {stations_csv}\")\n",
    "    stations.to_csv(stations_csv, index=False)\n",
    "    logger.success(\"Station datasets updated\")\n",
    "else:\n",
    "    logger.info(\n",
    "        \"OVERWRITE_DOWNLOADS is False; reusing pre-seeded station snapshot\"\n",
    "    )\n",
    "    if stations_parquet.exists():\n",
    "        stations = pd.read_parquet(stations_parquet)\n",
    "    elif stations_csv.exists():\n",
    "        stations = pd.read_csv(stations_csv)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"No station snapshot found in the seeded backup. Set OVERWRITE_DOWNLOADS=True to fetch fresh data.\"\n",
    "        )\n",
    "    logger.success(f\"Loaded station snapshot with {len(stations)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35f3539a-94cb-4121-8f38-61432bb177f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Scrape Mobi Website\n",
    "\n",
    "Many interesting questions arenâ€™t just about numbers â€” theyâ€™re about how the system is described to riders and stakeholders.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Optionally scrape selected pages from **https://www.mobibikes.ca/**.\n",
    "- Extract page titles, URLs, and main content into a structured table.\n",
    "- Prepare this content for use in **Vector Search** and **Genie rooms** (e.g. FAQ-style Q&A, policy lookups, marketing copy).\n",
    "\n",
    "Typical uses for this content in the hackathon:\n",
    "\n",
    "- Power a Genie room that can answer â€œhow does Mobi handle X?â€ using first-party content.\n",
    "- Enrich trip or station analyses with qualitative context from the website.\n",
    "\n",
    "> Scraping is intentionally lightweight and focused. Youâ€™re welcome to extend it, but keep an eye on depth and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8f25da-98bf-4070-9ae2-2e13ad0b0131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure scraper\n",
    "BASE_URL = CONFIG.get(\"scrape_url\")\n",
    "START_URL = BASE_URL\n",
    "SCRAPE_DELAY = float(CONFIG.get(\"scrape_delay\"))\n",
    "SCRAPE_MAX_DEPTH = int(CONFIG.get(\"scrape_max_depth\"))\n",
    "\n",
    "scraper = BasicSiteScraper(\n",
    "    base_url=BASE_URL,\n",
    "    delay=SCRAPE_DELAY,\n",
    "    max_depth=SCRAPE_MAX_DEPTH,\n",
    ")\n",
    "\n",
    "logger.info(f\"Scraping mobi site starting at {START_URL}\")\n",
    "\n",
    "if OVERWRITE_DOWNLOADS:\n",
    "    pages = scraper.scrape_recursive(START_URL)\n",
    "    logger.success(f\"Pages scraped: {len(pages)}\")\n",
    "else:\n",
    "    logger.info(\"OVERWRITE_DOWNLOADS is False; loading bundled site markdown\")\n",
    "    fallback_root = site_data_dir / \"raw\"\n",
    "    fallback_files = sorted(fallback_root.glob(\"*.md\"))\n",
    "    if not fallback_files:\n",
    "        raise RuntimeError(\n",
    "            \"No site markdown files available in the seeded backup. Set OVERWRITE_DOWNLOADS=True to scrape fresh content.\"\n",
    "        )\n",
    "\n",
    "    pages = {}\n",
    "    for path in fallback_files:\n",
    "        content = path.read_text(encoding=\"utf-8\")\n",
    "        url = f\"file:///{path.name}\"\n",
    "        pages[url] = {\n",
    "            \"metadata\": {\n",
    "                \"title\": path.stem.replace(\"_\", \" \").title(),\n",
    "                \"description\": \"\",\n",
    "                \"main_heading\": \"\",\n",
    "                \"url\": url,\n",
    "                \"scraped_at\": None,\n",
    "            },\n",
    "            \"content\": content,\n",
    "            \"links\": [],\n",
    "            \"status\": \"fallback\",\n",
    "        }\n",
    "\n",
    "    logger.success(f\"Loaded {len(pages)} pages from bundled markdown\")\n",
    "\n",
    "preview_urls = list(pages.keys())[: min(10, len(pages))]\n",
    "logger.info(f\"Previewing first {len(preview_urls)} URLs: {preview_urls}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2dd97a9-3a92-409f-a38f-fb3dc20722fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save markdown content per page\n",
    "raw_site_data_dir = site_data_dir / \"raw\"\n",
    "raw_site_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Persisting raw site markdown to {raw_site_data_dir}\")\n",
    "\n",
    "def url_to_filename(url: str) -> str:\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.rstrip(\"/\") or \"index\"\n",
    "    safe = re.sub(r\"[^a-zA-Z0-9._-]+\", \"_\", path)\n",
    "    if not safe.endswith(\".md\"):\n",
    "        safe += \".md\"\n",
    "    return safe\n",
    "\n",
    "count = 0\n",
    "for url, data in pages.items():\n",
    "    filepath = raw_site_data_dir / url_to_filename(url)\n",
    "    filepath.write_text(data.get(\"content\", \"\"), encoding=\"utf-8\")\n",
    "    count += 1\n",
    "\n",
    "logger.success(f\"Wrote {count} markdown files to {raw_site_data_dir}\")\n",
    "\n",
    "# Save to parquet\n",
    "records = []\n",
    "for url, data in pages.items():\n",
    "    md = data.get(\"metadata\", {})\n",
    "    records.append({\n",
    "        \"url\": url,\n",
    "        \"title\": md.get(\"title\", \"\"),\n",
    "        \"description\": md.get(\"description\", \"\"),\n",
    "        \"main_heading\": md.get(\"main_heading\", \"\"),\n",
    "        \"scraped_at\": md.get(\"scraped_at\", None),\n",
    "        \"status\": data.get(\"status\", \"\"),\n",
    "        \"content_md\": data.get(\"content\", \"\"),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "output_path = site_data_dir / \"mobibikes_ca_content.parquet\"\n",
    "logger.info(f\"Saving site content parquet to {output_path}\")\n",
    "save_to_parquet(df, output_path)\n",
    "logger.success(\"Site content parquet saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17347f2d-d4d9-40ea-a5eb-2a8f76da9e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load and Explore\n",
    "\n",
    "Once the trip CSVs are in your RAW volume, we load them into Spark and do a quick sanity check.\n",
    "\n",
    "In this section you will:\n",
    "\n",
    "- Read the raw CSVs into a DataFrame with appropriate options (header, delimiter, types).\n",
    "- Inspect basic schema and row counts.\n",
    "- Look for obvious data quality issues (missing columns, strange encodings, unexpected file names).\n",
    "\n",
    "This is a good moment to:\n",
    "\n",
    "- Note any quirks you may need to handle in your own feature engineering or ML models.\n",
    "- Decide which time ranges or subsets your team will focus on for the hackathon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd193f05-f988-46c2-8821-eeab66fa06ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load\n",
    "trips = pd.read_parquet(trip_data_dir / \"mobi_trips.parquet\")\n",
    "stations = pd.read_parquet(trip_data_dir / \"mobi_stations.parquet\")\n",
    "mobi_site = pd.read_parquet(site_data_dir / \"mobibikes_ca_content.parquet\")\n",
    "\n",
    "logger.success(f\"Trips loaded: {len(trips):,}\")\n",
    "logger.success(f\"Stations loaded: {len(stations)}\")\n",
    "logger.success(f\"Site pages available: {len(mobi_site)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf5664b0-a241-441f-ba41-79dc3baca2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Bronze Tables\n",
    "\n",
    "Bronze tables mirror the raw data as closely as possible while making it easier to query from SQL, Genie rooms, and notebooks.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Write the raw trip DataFrame into a **bronze Delta table** in Unity Catalog.\n",
    "- Preserve all raw columns and minimal transformations (e.g. consistent column naming, basic type casts).\n",
    "- Optionally create bronze tables for any additional raw inputs (stations, site content, etc.).\n",
    "\n",
    "At the end of this step you should have:\n",
    "\n",
    "- A `bronze_trips` table in your chosen `catalog.schema`.\n",
    "- A stable location you can use for exploration, feature building, and downstream silver pipelines.\n",
    "\n",
    "> Treat bronze as your â€œsingle source of raw truthâ€ â€” downstream tables can be rebuilt from it if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "656ee11b-627c-46bf-bf68-78b7cb529686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trips_pq = str(trip_data_dir / \"mobi_trips.parquet\")\n",
    "stations_pq = str(trip_data_dir / \"mobi_stations.parquet\")\n",
    "site_pq = str(site_data_dir / \"mobibikes_ca_content.parquet\")\n",
    "\n",
    "(\n",
    "    spark.read.parquet(trips_pq)\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", True)\n",
    "    .saveAsTable(f'`{CATALOG}`.`{SCHEMA}`.`bronze_trips`')\n",
    ")\n",
    "\n",
    "(\n",
    "    spark.read.parquet(stations_pq)\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", True)\n",
    "    .saveAsTable(f'`{CATALOG}`.`{SCHEMA}`.`bronze_stations`')\n",
    ")\n",
    "\n",
    "(\n",
    "    spark.read.parquet(site_pq)\n",
    "    .write.format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", True)\n",
    "    .saveAsTable(f'`{CATALOG}`.`{SCHEMA}`.`bronze_site`')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be2a9131-bf81-4617-bc22-ece172fdb0cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Silver Tables\n",
    "\n",
    "### What Are Silver Tables?\n",
    "\n",
    "Databricks workflows commonly organize data into three layers:\n",
    "\n",
    "- **Bronze**  \n",
    "  Raw or minimally processed data â€” faithful to the source, high-volume, low-governance.\n",
    "\n",
    "- **Silver**  \n",
    "  Cleaned, structured, well-typed, and analytics/ML-ready data.  \n",
    "  Columns are standardized. Types are consistent. IDs are stable. Missing or inconsistent values are handled.\n",
    "\n",
    "- **Gold** (not used in this repo)  \n",
    "  Business-level aggregates, feature tables, or curated outputs.\n",
    "\n",
    "In this project, you will create **three Silver tables**, each with a different purpose:\n",
    "\n",
    "1. **`silver_trips`** â€” enriched, normalized trip history  \n",
    "2. **`silver_stations`** â€” a cleaned station reference table  \n",
    "3. **`silver_site_pages`** â€” structured website content ready for embeddings and vector search\n",
    "\n",
    "Silver tables matter because:\n",
    "\n",
    "- They remove irregularities from raw input sources  \n",
    "- They provide clean schemas for use by Genie, ML models, BI tools, and vector search  \n",
    "- They offer consistent join keys across datasets  \n",
    "- They are the *primary* tables most teams will query during the hackathon  \n",
    "\n",
    "Think of silver tables as your **default working layer** â€”  \n",
    "Bronze is for ingestion, Silver is for intelligence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eae82f17-3dbf-4abf-9e01-637bdb6e8cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspect bronze tables before silver transforms\n",
    "\n",
    "def summarize_columns(df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a sorted DataFrame detailing column names, dtypes, and null counts.\"\"\"\n",
    "    summary = pd.DataFrame(\n",
    "        {\n",
    "            \"column\": df.columns,\n",
    "            \"dtype\": df.dtypes.astype(str),\n",
    "            \"null_count\": df.isna().sum(),\n",
    "        }\n",
    "    )\n",
    "    logger.info(f\"{label} has {len(df):,} rows\")\n",
    "    return summary.sort_values(\"column\").reset_index(drop=True)\n",
    "\n",
    "bronze_trips_summary = summarize_columns(trips, \"bronze_trips\")\n",
    "bronze_stations_summary = summarize_columns(stations, \"bronze_stations\")\n",
    "bronze_site_summary = summarize_columns(mobi_site, \"bronze_site\")\n",
    "\n",
    "display(bronze_trips_summary)\n",
    "display(bronze_stations_summary)\n",
    "display(bronze_site_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0108495-0d46-44b3-b051-41f75412b065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper utilities for silver preparation\n",
    "\n",
    "def clean_station_label_col(column: F.Column) -> F.Column:\n",
    "    \"\"\"Return a Spark column with 4-digit numeric prefixes removed and whitespace normalized.\"\"\"\n",
    "    return F.regexp_replace(\n",
    "        F.regexp_replace(F.trim(column), r\"^\\s*\\d{4}[\\s\\-:]*\", \"\"), r\"\\s+\", \" \"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_station_key_col(column: F.Column) -> F.Column:\n",
    "    \"\"\"Create a lowercase, single-space station key suitable for joining.\"\"\"\n",
    "    return F.lower(F.regexp_replace(F.trim(column), r\"\\s+\", \" \"))\n",
    "\n",
    "\n",
    "def coerce_flag_col(column: F.Column) -> F.Column:\n",
    "    \"\"\"Convert yes/no style strings to a nullable boolean Spark column.\n",
    "\n",
    "    TODO: Extend with additional localized values if they appear in new extracts.\n",
    "    \"\"\"\n",
    "    normalized = F.lower(F.trim(column))\n",
    "    truthy = [\"yes\", \"true\", \"1\", \"y\"]\n",
    "    falsy = [\"no\", \"false\", \"0\", \"n\"]\n",
    "    return (\n",
    "        F.when(normalized.isin(*truthy), F.lit(True))\n",
    "        .when(normalized.isin(*falsy), F.lit(False))\n",
    "        .otherwise(F.lit(None))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ff74fb-2d58-472d-a12f-b63bcdc493da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Silver Stations Table\n",
    "\n",
    "The raw GBFS feeds provide a mixture of static station metadata (â€œstation informationâ€) and dynamic availability snapshots (â€œstation statusâ€).  \n",
    "To simplify downstream use, we consolidate these sources into a clean and consistent **`silver_stations`** reference table.\n",
    "\n",
    "This table includes:\n",
    "\n",
    "- Station IDs  \n",
    "- Human-readable names  \n",
    "- Latitude/longitude coordinates  \n",
    "- Dock capacity  \n",
    "- Normalized station names/labels  \n",
    "- Any useful attributes extracted from scraped pages (if applicable)\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "- Station names are inconsistent in the raw data (prefixes, formatting issues, multiple naming conventions)\n",
    "- Trip datasets rely on station names/IDs for joins â€” consistency is critical\n",
    "- Mapping, ML features, and agent workflows depend on precise geospatial information\n",
    "\n",
    "`silver_stations` acts as the **source of truth** for anything involving station identity, location, or metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a07c4a7-c45e-4847-8d3a-370d4bcf7e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare silver_stations DataFrame using Spark\n",
    "bronze_stations_sdf = spark.table(f'`{CATALOG}`.`{SCHEMA}`.`bronze_stations`')\n",
    "\n",
    "silver_stations = (\n",
    "    bronze_stations_sdf\n",
    "    .withColumn(\"station_name\", clean_station_label_col(F.col(\"name\")))\n",
    "    .withColumn(\"station_key\", normalize_station_key_col(F.col(\"name\")))\n",
    "    .withColumn(\"station_pk\", F.col(\"station_id\"))\n",
    "    .dropDuplicates([\"station_id\"])\n",
    ")\n",
    "# TODO: capture additional station metadata (e.g., capacity) if required downstream.\n",
    "logger.success(f\"Prepared silver_stations with {silver_stations.count()} rows\")\n",
    "display(silver_stations.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54e33152-6bad-49b9-9d01-688912579d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Silver Trips Table\n",
    "\n",
    "Our bronze trips table mirrors the raw CSVs, which is great for fidelity but not ideal for everyday analytics or ML. Silver tables give you a cleaner, more opinionated view.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Transform `bronze_trips` into a **`silver_trips`** table designed for exploration, modelling, and Genie.\n",
    "- Apply a focused set of transformations:\n",
    "  - **Normalize columns** â€“ consolidate duplicate or inconsistent names, cast metrics to numeric types, and standardize timestamps.\n",
    "  - **Derive flags and features** â€“ e.g. a single, trusted indicator for electric bikes.\n",
    "  - **Standardize station names/IDs** â€“ strip numeric prefixes where appropriate and ensure both start and end stations are easy to join.\n",
    "  - **Assign primary keys** â€“ create a deterministic `trip_id` using a window function so every trip row has a unique identifier.\n",
    "\n",
    "You should:\n",
    "\n",
    "- Inspect sample rows from both bronze and silver to see exactly what changed.\n",
    "- Decide whether to add your own features (time-of-day, day-of-week, weather joins, etc.) either here or in your own notebooks.\n",
    "\n",
    "> Think of `silver_trips` as the default table your team uses for queries, dashboards, agents, and ML â€” bronze is there when you need to go back to the raw source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ffd23db-13d5-416f-9e2c-a3305c0f79bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load bronze trips as the starting point for the silver transformation\n",
    "bronze_trips_sdf = spark.table(f'`{CATALOG}`.`{SCHEMA}`.`bronze_trips`')\n",
    "silver_trips = bronze_trips_sdf\n",
    "logger.info(f\"Loaded bronze_trips rows: {silver_trips.count()}\")\n",
    "display(silver_trips.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af50496a-dea8-4303-9e26-5d18a989ddd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Coalesce duplicate membership columns\n",
    "if \"memebership_type\" in bronze_trips_sdf.columns:\n",
    "    if \"membership_type\" in silver_trips.columns:\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            \"membership_type\",\n",
    "            F.coalesce(F.col(\"membership_type\"), F.col(\"memebership_type\")),\n",
    "        )\n",
    "    else:\n",
    "        silver_trips = silver_trips.withColumn(\"membership_type\", F.col(\"memebership_type\"))\n",
    "    silver_trips = silver_trips.drop(\"memebership_type\")\n",
    "\n",
    "logger.info(f\"After membership cleanup: {silver_trips.count()} rows\")\n",
    "display(silver_trips.select(\"membership_type\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb736ed3-3af3-4fe7-983f-33c1d698b037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Normalize stopover duration values\n",
    "if \"stopover_duration\" in silver_trips.columns:\n",
    "    if \"stopover_duration_sec\" in silver_trips.columns:\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            \"stopover_duration_sec\",\n",
    "            F.coalesce(F.col(\"stopover_duration_sec\"), F.col(\"stopover_duration\")),\n",
    "        )\n",
    "    else:\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            \"stopover_duration_sec\", F.col(\"stopover_duration\").cast(\"double\")\n",
    "        )\n",
    "    silver_trips = silver_trips.drop(\"stopover_duration\")\n",
    "\n",
    "logger.info(f\"After stopover normalization: {silver_trips.count()} rows\")\n",
    "display(silver_trips.select(\"stopover_duration_sec\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "276f8149-388e-4686-bb62-bbbf648df246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Apply numeric casts and derived distance\n",
    "numeric_casts = {\n",
    "    \"bike_id\": \"bigint\",\n",
    "    \"covered_distance_m\": \"double\",\n",
    "    \"duration_sec\": \"double\",\n",
    "    \"stopover_duration_sec\": \"double\",\n",
    "    \"number_of_stopovers\": \"double\",\n",
    "    \"departure_slot\": \"double\",\n",
    "    \"return_slot\": \"double\",\n",
    "    \"lock_duration_sec\": \"double\",\n",
    "    \"number_of_bike_locks\": \"double\",\n",
    "    \"number_of_bike_stopovers\": \"double\",\n",
    "    \"departure_battery_voltage_mv\": \"double\",\n",
    "    \"return_battery_voltage_mv\": \"double\",\n",
    "}\n",
    "for column, dtype in numeric_casts.items():\n",
    "    if column in silver_trips.columns:\n",
    "        silver_trips = silver_trips.withColumn(column, F.col(column).cast(dtype))\n",
    "\n",
    "if \"covered_distance_m\" in silver_trips.columns:\n",
    "    silver_trips = silver_trips.withColumn(\n",
    "        \"covered_distance_km\", F.col(\"covered_distance_m\") / F.lit(1000.0)\n",
    "    )\n",
    "\n",
    "logger.info(f\"After numeric coercion: {silver_trips.count()} rows\")\n",
    "display(silver_trips.select(*[c for c in [\"covered_distance_m\", \"covered_distance_km\", \"duration_sec\"] if c in silver_trips.columns]).limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad4ca5fe-9d8f-4a3f-86ad-5e75e3182a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Cast departure and return timestamps (source data already hourly)\n",
    "# Handle slash and dash formats plus minute-only timestamps without seconds\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# Pandas UDF fallback\n",
    "@pandas_udf(TimestampType())\n",
    "def infer_ts(col: pd.Series) -> pd.Series:\n",
    "    \"\"\"Fallback parser using pandas.to_datetime() with inference.\"\"\"\n",
    "    return pd.to_datetime(col, errors=\"coerce\", infer_datetime_format=True)\n",
    "\n",
    "# Fast native parse\n",
    "def fast_timestamp(col):\n",
    "    \"\"\"Lightweight parser for most rows.\"\"\"\n",
    "    c = F.trim(F.col(col))\n",
    "    c = F.regexp_replace(c, r\"[./]\", \"-\")  # normalize separators\n",
    "    # reorder M-D-Y -> Y-M-D\n",
    "    c = F.regexp_replace(c, r\"^([0-9]{1,2})-([0-9]{1,2})-([0-9]{4})\", r\"\\3-\\1-\\2\")\n",
    "    return F.try_to_timestamp(c)\n",
    "\n",
    "for raw, target in [(\"departure\", \"departure_time\"), (\"return\", \"return_time\")]:\n",
    "    if raw in silver_trips.columns:\n",
    "        # fast native parse\n",
    "        silver_trips = silver_trips.withColumn(target, fast_timestamp(raw))\n",
    "        # pandas fallback only for nulls\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            target,\n",
    "            F.when(F.col(target).isNull(), infer_ts(F.col(raw))).otherwise(F.col(target))\n",
    "        )\n",
    "        silver_trips = silver_trips.drop(raw)\n",
    "\n",
    "for colname in [\"departure_time\", \"return_time\"]:\n",
    "    if colname in silver_trips.columns:\n",
    "        silver_trips = silver_trips.withColumn(colname, fast_timestamp(colname))\n",
    "        silver_trips = silver_trips.withColumn(\n",
    "            colname,\n",
    "            F.when(F.col(colname).isNull(), infer_ts(F.col(colname))).otherwise(F.col(colname))\n",
    "        )\n",
    "\n",
    "# Add date based partitioning columns for later\n",
    "if \"departure_time\" in silver_trips.columns:\n",
    "    silver_trips = (\n",
    "        silver_trips\n",
    "        .withColumn(\"departure_year\",  F.year(\"departure_time\"))\n",
    "        .withColumn(\"departure_month\", F.month(\"departure_time\"))\n",
    "    )\n",
    "\n",
    "display(silver_trips.select(\"departure_time\", \"return_time\", \"departure_year\", \"departure_month\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c595345-f507-4917-9d83-2386dda92280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Consolidate electric bike indicators (coalesce known boolean columns)\n",
    "flag_columns = [c for c in [\"electric_bike\", \"electric\"] if c in silver_trips.columns]\n",
    "\n",
    "if flag_columns:\n",
    "    silver_trips = silver_trips.withColumn(\n",
    "        \"is_electric_bike\",\n",
    "        F.coalesce(*[F.col(c).cast(\"boolean\") for c in flag_columns]),\n",
    "    )\n",
    "else:\n",
    "    silver_trips = silver_trips.withColumn(\"is_electric_bike\", F.lit(None).cast(\"boolean\"))\n",
    "\n",
    "for column in flag_columns:\n",
    "    silver_trips = silver_trips.drop(column)\n",
    "\n",
    "logger.info(f\"After electric flag normalization: {silver_trips.count()} rows\")\n",
    "display(silver_trips.select(\"is_electric_bike\").limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63b708c7-e4a2-44a8-83ca-feea6a00571e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Clean station names (strip numeric prefixes, collapse whitespace)\n",
    "for prefix in [\"departure\", \"return\"]:\n",
    "    raw_column = f\"{prefix}_station\"\n",
    "    if raw_column in silver_trips.columns:\n",
    "        cleaned = clean_station_label_col(F.col(raw_column))\n",
    "        silver_trips = silver_trips.withColumn(f\"{prefix}_station_name\", cleaned)\n",
    "        silver_trips = silver_trips.drop(raw_column)\n",
    "\n",
    "logger.info(f\"After station name cleanup: {silver_trips.count()} rows\")\n",
    "display(\n",
    "    silver_trips.select(\n",
    "        *[\n",
    "            c\n",
    "            for c in [\n",
    "                \"departure_station_name\",\n",
    "                \"return_station_name\",\n",
    "            ]\n",
    "            if c in silver_trips.columns\n",
    "        ]\n",
    "    ).limit(5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17496c1e-e1ae-4b45-ba0a-ff4b6cef00fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: Attach station IDs using normalized names\n",
    "dep_lookup = silver_stations.select(\n",
    "    F.col(\"station_key\").alias(\"dep_station_key\"),\n",
    "    F.col(\"station_id\").alias(\"departure_station_id\"),\n",
    ")\n",
    "ret_lookup = silver_stations.select(\n",
    "    F.col(\"station_key\").alias(\"ret_station_key\"),\n",
    "    F.col(\"station_id\").alias(\"return_station_id\"),\n",
    ")\n",
    "\n",
    "silver_trips = silver_trips.join(\n",
    "    dep_lookup,\n",
    "    normalize_station_key_col(F.col(\"departure_station_name\")) == dep_lookup[\"dep_station_key\"],\n",
    "    \"left\",\n",
    ").drop(\"dep_station_key\")\n",
    "\n",
    "silver_trips = silver_trips.join(\n",
    "    ret_lookup,\n",
    "    normalize_station_key_col(F.col(\"return_station_name\")) == ret_lookup[\"ret_station_key\"],\n",
    "    \"left\",\n",
    ").drop(\"ret_station_key\")\n",
    "\n",
    "logger.info(f\"After station ID join: {silver_trips.count()} rows\")\n",
    "display(\n",
    "    silver_trips.select(\n",
    "        *[\n",
    "            c\n",
    "            for c in [\n",
    "                \"departure_station_name\",\n",
    "                \"departure_station_id\",\n",
    "                \"return_station_name\",\n",
    "                \"return_station_id\",\n",
    "            ]\n",
    "            if c in silver_trips.columns\n",
    "        ]\n",
    "    ).limit(5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46599b20-d845-47fa-b397-1d4c4f9d2feb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 8: Finalize trip_id and column ordering (simple fast version)\n",
    "\n",
    "# Generate a unique ID per row\n",
    "silver_trips = silver_trips.withColumn(\"trip_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Preserve original column ordering logic\n",
    "trip_columns_order = [\n",
    "    \"trip_id\",\n",
    "    \"departure_time\",\n",
    "    \"return_time\",\n",
    "    \"bike\",\n",
    "    \"is_electric_bike\",\n",
    "    \"membership_type\",\n",
    "    \"covered_distance_m\",\n",
    "    \"covered_distance_km\",\n",
    "    \"duration_sec\",\n",
    "    \"stopover_duration_sec\",\n",
    "    \"number_of_stopovers\",\n",
    "    \"number_of_bike_stopovers\",\n",
    "    \"departure_slot\",\n",
    "    \"return_slot\",\n",
    "    \"lock_duration_sec\",\n",
    "    \"number_of_bike_locks\",\n",
    "    \"departure_temperature_c\",\n",
    "    \"return_temperature_c\",\n",
    "    \"departure_battery_voltage_mv\",\n",
    "    \"return_battery_voltage_mv\",\n",
    "    \"departure_station_name\",\n",
    "    \"departure_station_id\",\n",
    "    \"return_station_name\",\n",
    "    \"return_station_id\",\n",
    "    \"source_file\",\n",
    "    \"formula\",\n",
    "    \"manager\",\n",
    "]\n",
    "\n",
    "existing_trip_columns = [c for c in trip_columns_order if c in silver_trips.columns]\n",
    "remaining_columns = [c for c in silver_trips.columns if c not in existing_trip_columns]\n",
    "silver_trips = silver_trips.select(*(existing_trip_columns + remaining_columns))\n",
    "\n",
    "logger.success(f\"Prepared silver_trips with {silver_trips.count()} rows\")\n",
    "display(silver_trips.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6fd6ec5-bc2a-41b9-b9b8-7b678619b424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Silver Site Table\n",
    "\n",
    "The scraped website pages contain rich textual content that can power:\n",
    "\n",
    "- Semantic search  \n",
    "- Retrieval-augmented generation  \n",
    "- Question answering in Genie rooms  \n",
    "- Summaries, descriptions, and contextual reasoning for ML or agent workflows  \n",
    "\n",
    "The raw scraped output can be noisy, so here we build **`silver_site_pages`**, which includes:\n",
    "\n",
    "- Clean page titles  \n",
    "- Normalized URLs  \n",
    "- Markdown-formatted content (`content_md`)  \n",
    "- Page categories or implicit tags (optional)\n",
    "\n",
    "This table is structured specifically to support **Vector Search**, where each row becomes:\n",
    "\n",
    "- An embedding  \n",
    "- A retrievable document  \n",
    "- A source for agent answers or table function inputs  \n",
    "\n",
    "If your team plans to use GenAI or build QA-style features, this table will be one of your most important assets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ad73341-246f-4983-9c41-1b719f8ec69e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare silver_site DataFrame and assign primary key\n",
    "bronze_site_sdf = spark.table(f'`{CATALOG}`.`{SCHEMA}`.`bronze_site`')\n",
    "\n",
    "site_window = Window.orderBy(\"url\")\n",
    "silver_site = (\n",
    "    bronze_site_sdf\n",
    "    .withColumn(\"scraped_at\", F.to_timestamp(\"scraped_at\"))\n",
    "    .dropDuplicates([\"url\"])\n",
    "    .withColumn(\"site_page_id\", F.row_number().over(site_window).cast(\"bigint\"))\n",
    ")\n",
    "# TODO: revisit primary key strategy if additional natural keys become available.\n",
    "\n",
    "logger.success(f\"Prepared silver_site with {silver_site.count()} pages\")\n",
    "display(silver_site.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbc6588-d1c4-4740-8624-78047bd62271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Persist silver tables to Unity Catalog Delta\n",
    "\n",
    "logger.info(\"Repartitioning silver_trips by year and month\")\n",
    "silver_trips = silver_trips.repartition(\"departure_year\", \"departure_month\")\n",
    "\n",
    "# Drop existing tables (if any) to avoid slow schema reconciliation\n",
    "for t in [\"silver_trips\", \"silver_stations\", \"silver_site\"]:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS `{CATALOG}`.`{SCHEMA}`.`{t}`\")\n",
    "\n",
    "logger.info(f\"Writing silver_trips to `{CATALOG}`.`{SCHEMA}`.silver_trips\")\n",
    "(\n",
    "    silver_trips.write\n",
    "    .format(\"delta\")\n",
    "    .partitionBy(\"departure_year\", \"departure_month\")\n",
    "    .saveAsTable(f\"`{CATALOG}`.`{SCHEMA}`.`silver_trips`\")\n",
    ")\n",
    "\n",
    "logger.info(f\"Writing silver_stations to `{CATALOG}`.`{SCHEMA}`.silver_stations\")\n",
    "(\n",
    "    silver_stations.write\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(f\"`{CATALOG}`.`{SCHEMA}`.`silver_stations`\")\n",
    ")\n",
    "\n",
    "logger.info(f\"Writing silver_site to `{CATALOG}`.`{SCHEMA}`.silver_site\")\n",
    "(\n",
    "    silver_site.write\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(f\"`{CATALOG}`.`{SCHEMA}`.`silver_site`\")\n",
    ")\n",
    "\n",
    "logger.success(\"Silver tables materialized\")\n",
    "\n",
    "silver_trips.printSchema()\n",
    "silver_stations.printSchema()\n",
    "silver_site.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2018cda6-06c7-41dd-89bf-259dac374111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7783426245318998,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
